{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM SRGAN on analogous dataset\n",
    "https://github.com/monishramadoss/SRGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SRGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hyperparameters\n",
    "UPSCALE_FACTOR = 2\n",
    "BATCHSIZE = 2\n",
    "EPOCHS = 1000\n",
    "LOWRES = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "############  setup all of the data imports ############]\n",
    "DEVSET_URL = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip\"\n",
    "TRAINSET_URL = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
    "DEVSET = \"./data/DIV2K_valid_HR.zip\"\n",
    "TRAINSET = \"./data/DIV2K_train_HR.zip\"\n",
    "DEVDATA_FOLDER = \"./data/DIV2K_valid_HR\"\n",
    "TRAINDATA_FOLDER = \"./data/DIV2K_train_HR\"\n",
    "\n",
    "##makeshift printer class, since not using keras##\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "        \n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "if not os.path.exists('./data/train'):\n",
    "    os.makedirs('./data/train')\n",
    "if not os.path.exists('./data/dev'):\n",
    "    os.makedirs('./data/dev')\n",
    "if not os.path.exists('./checkpoint'):\n",
    "    os.makedirs('./checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import all the torch modules\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setup swish function\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "#Setup a class that holds nn.Sequential(features)\n",
    "#Has a function call forward, returns the features a certain x has\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, feature_layer=11):\n",
    "        ##concatinating model, latter part of the model is for categorization. All we need is what features in image\n",
    "        ##just want the features for all images\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn = torchvision.models.vgg19(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(cnn.features.children())[:feature_layer+1])\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Discriminator class. This will be trained to say true or false if something is real.\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)\n",
    "        \n",
    "        ##Vgg16\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##forward propogation\n",
    "        x = swish(self.conv1(x))\n",
    "        x = swish(self.bn2(self.conv2(x)))\n",
    "        x = swish(self.bn3(self.conv3(x)))\n",
    "        x = swish(self.bn4(self.conv4(x)))\n",
    "        x = swish(self.bn5(self.conv5(x)))\n",
    "        x = swish(self.bn6(self.conv6(x)))\n",
    "        x = swish(self.bn7(self.conv7(x)))\n",
    "        x = swish(self.bn8(self.conv8(x)))\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        return torch.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this class generates fake images\n",
    "##Resnet 50 architecture\n",
    "##last few blocks for upsampling\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_residual_blocks, upscale_factor=2, n_filters=64, inplace=False):\n",
    "        super(Generator, self).__init__()\n",
    "        ##init some variables\n",
    "        self.n_residual_blocks = n_residual_blocks\n",
    "        self.upsample_factor = upscale_factor\n",
    "        ##Setup a Conv2d initial layer\n",
    "        self.conv1 = nn.Conv2d(3, n_filters, 9, stride=1, padding=4)\n",
    "\n",
    "        for i in range(self.n_residual_blocks):\n",
    "            self.add_module('residual_block' + str(i + 1), ResidualBlock(n_filters, 3, n_filters, 1))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(n_filters, n_filters, 3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(n_filters)\n",
    "        for i in range(self.upsample_factor // 2):\n",
    "            self.add_module('upsample' + str(i + 1), UpsampleBlock(n_filters, n_filters))\n",
    "        self.conv3 = nn.Conv2d(n_filters, 3, 9, stride=1, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = swish(self.conv1(x))\n",
    "        y = x.clone()\n",
    "\n",
    "        for i in range(self.n_residual_blocks):\n",
    "            y = self.__getattr__('residual_block' + str(i + 1))(y)\n",
    "\n",
    "        x = self.bn2(self.conv2(y)) + x\n",
    "\n",
    "        for i in range(self.upsample_factor // 2):\n",
    "            x = self.__getattr__('upsample' + str(i + 1))(x)\n",
    "\n",
    "        return self.conv3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Subpixel convolution\n",
    "##where we want to upsample(increase resolution)\n",
    "##Convolution transpose very slow, alternative is subpixel convolution\n",
    "class UpSampleConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=2):\n",
    "        super(UpSampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.upsample_layer = nn.Upsample(scale_factor=upsample)\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.upsample_layer(x)\n",
    "        y = self.reflection_pad(y)\n",
    "        y = self.conv(y)\n",
    "        return y\n",
    "\n",
    "##This is a resnet residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels=64, k=3, n=64, s=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, n, k, stride=s, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(n)\n",
    "        self.conv2 = nn.Conv2d(n, n, k, stride=s, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = swish(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = y + x\n",
    "        return y\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        # self.conv = nn.Conv2d(in_channels, out_channels * 4, 3, 1, padding=1)\n",
    "        self.convT = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        # self.up_layer = UpSampleConvLayer(in_channels, out_channels, 3, 1)\n",
    "        # self.shuffler = nn.PixelShuffle(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # y = self.up_layer(x)\n",
    "        # y = self.conv(x)\n",
    "        y = self.convT(x)\n",
    "        # y = self.shuffler(y)\n",
    "        y = self.bn(y)\n",
    "        y = swish(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: /Users/vinayak/Documents/SRGAN_Research_Sharma/SRGAN_tester/data/train\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-afe23e996db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                             transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Users/vinayak/Documents/SRGAN_Research_Sharma/SRGAN_tester/data/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdev_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Users/vinayak/Documents/SRGAN_Research_Sharma/SRGAN_tester/data/dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     ):\n\u001b[0;32m--> 226\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Supported extensions are: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: /Users/vinayak/Documents/SRGAN_Research_Sharma/SRGAN_tester/data/train\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomCrop(LOWRES*UPSCALE_FACTOR),\n",
    "                                transforms.ToTensor()])\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "unnormalize = transforms.Normalize(mean = [-2.118, -2.036, -1.804], std = [4.367, 4.464, 4.444])\n",
    "\n",
    "scale = transforms.Compose([transforms.ToPILImage(), transforms.Resize(LOWRES),\n",
    "                            transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='/Users/vinayak/Documents/SRGAN_Research_Sharma/SRGAN_tester/data/train', transform=transform)\n",
    "dev_dataset = datasets.ImageFolder(root='/Users/vinayak/Documents/SRGAN_Research_Sharma/SRGAN_tester/data/dev', transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, BATCHSIZE)\n",
    "valid_dataloader = torch.utils.data.DataLoader(dev_dataset, 1)\n",
    "\n",
    "content_criterion = nn.MSELoss()\n",
    "# GeneratorDevice = torch.device(\"cuda:0\")\n",
    "# DiscriminatorDevice = torch.device(\"cuda:0\")\n",
    "GeneratorDevice = torch.device(\"cpu\")\n",
    "DiscriminatorDevice = torch.device(\"cpu\")\n",
    "adversarial_criterion = nn.BCELoss()\n",
    "\n",
    "generator = Generator(16, UPSCALE_FACTOR)\n",
    "discriminator = Discriminator()\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "generator = generator.to(GeneratorDevice)\n",
    "discriminator = discriminator.to(DiscriminatorDevice)\n",
    "feature_extractor = feature_extractor.to(DiscriminatorDevice)\n",
    "low_res = torch.FloatTensor(BATCHSIZE, 3, LOWRES, LOWRES)\n",
    "ones_const = torch.ones(BATCHSIZE, 1).to(DiscriminatorDevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreTrain Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6f673bcd9b434c862792fd3eff0492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pretraining', max=2, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optim_generator = optim.Adam(generator.parameters(), lr=0.0001)\n",
    "for epoch in tqdm(range(2), desc = \"Pretraining\"):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aa51cf507e412781f10d035e7b1f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='pretraining', max=2, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optim_generator = optim.Adam(generator.parameters(), lr=0.0001)\n",
    "for epoch in tqdm(range(2), desc ='pretraining'):\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            high_res_real, _ = data \n",
    "            for j in range(BATCHSIZE):\n",
    "                low_res[j] = scale(high_res_real[j])\n",
    "                high_res_real[j] = normalize(high_res_real[j])\n",
    "\n",
    "            high_res_real = high_res_real.to(GeneratorDevice)\n",
    "            high_res_fake = generator(low_res.to(GeneratorDevice))\n",
    "\n",
    "            generator.zero_grad()\n",
    "            generator_content_loss = content_criterion(high_res_fake, high_res_real)\n",
    "            generator_content_loss.backward()\n",
    "            optim_generator.step()\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboardX'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-02a11a0fd3a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-9dc82e897315>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-9dc82e897315>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for epoch in tqdm(range(EPOCH), desc = ):\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(\"SRGAN Training\")\n",
    "\n",
    "    for epoch in tqdm(range(500)):\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            high_res_real, _ = data\n",
    "            if high_res_real.shape[0] == batchSize:\n",
    "                for j in range(batchSize):\n",
    "                    low_res[j] = dataloader.scale(high_res_real[j])\n",
    "                    high_res_real[j] = dataloader.normalize(high_res_real[j])\n",
    "\n",
    "                high_res_real = high_res_real.to(GeneratorDevice)\n",
    "                high_res_fake = generator(low_res.to(GeneratorDevice))\n",
    "\n",
    "                target_real = (torch.rand(batchSize, 1) * 0.5 + 0.7).to(DiscriminatorDevice)\n",
    "                target_fake = (torch.rand(batchSize, 1) * 0.3).to(DiscriminatorDevice)\n",
    "                high_res_fake = high_res_fake.to(DiscriminatorDevice)\n",
    "                high_res_real = high_res_real.to(DiscriminatorDevice)\n",
    "\n",
    "                # Train Discriminator#\n",
    "                discriminator.zero_grad()\n",
    "                discriminiator_loss = adversarial_criterion(discriminator(high_res_real), target_real) + adversarial_criterion(discriminator(high_res_fake), target_fake)\n",
    "                discriminiator_loss.backward(retain_graph=True)\n",
    "                discriminator_optimzer.step()\n",
    "                real_features = feature_extractor(high_res_real)\n",
    "                fake_features = feature_extractor(high_res_fake)\n",
    "\n",
    "                # Train Generator#\n",
    "                generator.zero_grad()\n",
    "                generator_content_loss = content_criterion(high_res_fake, high_res_real) + 0.006 * content_criterion(fake_features, real_features)\n",
    "                generator_adversarial_loss = adversarial_criterion(discriminator(high_res_fake), ones_const)\n",
    "                generator_total_loss = generator_content_loss + 0.001 * generator_adversarial_loss\n",
    "                generator_total_loss.backward()\n",
    "                generator_optimzer.step()\n",
    "\n",
    "                writer.add_scalar('data/generator_content_loss,', generator_content_loss, count)\n",
    "                writer.add_scalar('data/generator_adversarial_loss,', generator_adversarial_loss, count)\n",
    "                writer.add_scalar('data/generator_total_loss,', generator_total_loss, count)\n",
    "                writer.add_scalar('data/discriminator_loss,', discriminiator_loss, count)\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
